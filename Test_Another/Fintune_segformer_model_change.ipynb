{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import glob\n",
    "import cv2 as cv\n",
    "import random\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "import PIL.ImageOps    \n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.utils\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import timm\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RLE 인코딩 함수\n",
    "def rle_encode(mask):\n",
    "    pixels = mask.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = 'a'\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "CFG = {\n",
    "    'IMG_SIZE':224,\n",
    "    'EPOCHS':20,\n",
    "    'LEARNING_RATE':3e-5,\n",
    "    # 'LEARNING_RATE':10,\n",
    "    'BATCH_SIZE':8,\n",
    "    'SEED':41\n",
    "}\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#폴더 이동시 경로 수정이 필요할 수 있음 \n",
    "train_source = glob.glob(\"../Data/train_source_image/*\")\n",
    "val_source = glob.glob(\"../Data/val_source_image/*\")\n",
    "train_gt = glob.glob(\"../Data/train_source_gt/*\")\n",
    "val_gt = glob.glob(\"../Data/val_source_gt/*\")\n",
    "\n",
    "train_source += val_source\n",
    "train_gt += val_gt\n",
    "\n",
    "# glob 이후에 정렬이 안되어 있기 때문에, source - gt matching을 위해 정렬\n",
    "train_source.sort()\n",
    "train_gt.sort()\n",
    "\n",
    "print(train_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF 생성 \n",
    "df = pd.DataFrame(columns=['source','gt'])\n",
    "df['source'] = train_source\n",
    "df['gt'] = train_gt\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, source, gt, transform=None,t2= None, infer=False):\n",
    "        self.source = source\n",
    "        self.gt = gt\n",
    "        self.transform = transform\n",
    "        self.t2 = t2\n",
    "        self.infer = infer\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.source[idx]\n",
    "        image = cv.imread(img_path)\n",
    "        \n",
    "        \n",
    "        if self.infer:\n",
    "            label = self.gt[idx]\n",
    "            if self.transform:\n",
    "                image = self.transform(image=image)['image']\n",
    "            return image, label\n",
    "        \n",
    "        mask_path = self.gt[idx]\n",
    "        mask = cv.imread(mask_path, cv.IMREAD_GRAYSCALE)\n",
    "        mask[mask == 255] = 12 #/ 배경을 픽셀값 12로 간주 이거 원래 없던 값!\n",
    "\n",
    "        if self.transform: # 알부네이션 먹이이는 형식으로 진행 \n",
    "            augmented = self.transform(image=image, mask = mask) \n",
    "            image = augmented['image']\n",
    "            \n",
    "        if self.t2: #현재 들어가는 구조가 다름 totensor 진행해줌\n",
    "            augmented = self.t2(image=image, mask = mask) \n",
    "            mask = augmented['mask']\n",
    "            \n",
    "            \n",
    "        return image, mask\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.source)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfrom - Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose(\n",
    "    [   \n",
    "        A.Resize(540, 960),\n",
    "        A.Normalize(),\n",
    "        # ToTensorV2()\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_gt = A.Compose(\n",
    "    [   \n",
    "        A.Resize(128, 256), # 반대일 수도 있음 \n",
    "        A.Normalize(),\n",
    "        ToTensorV2()\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, _, _ = train_test_split(df, _, test_size=0.3, random_state=CFG['SEED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(source = train['source'].values, gt = train['gt'].values, transform=None,t2 = transform_gt, infer=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "\n",
    "val_dataset = CustomDataset(source = val['source'].values, gt = val['gt'].values, transform=None,t2 = transform_gt, infer=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "### SMP API\n",
    "\n",
    "- model.encoder - pretrained backbone to extract features of different spatial resolution\n",
    "- model.decoder - depends on models architecture (Unet/Linknet/PSPNet/FPN)\n",
    "- model.segmentation_head - last block to produce required number of mask channels (include also optional upsampling and activation)\n",
    "- model.classification_head - optional block which create classification head on top of encoder\n",
    "- model.forward(x) - sequentially pass x through model`s encoder, decoder and segmentation head (and classification head if specified)\n",
    "\n",
    "### Model Param\n",
    " - Docs - https://www.kaggle.com/code/ligtfeather/semantic-segmentation-is-easy-with-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\n",
    "from PIL import Image\n",
    "import requests\n",
    "import cv2 as cv\n",
    "\n",
    "feature_extractor = SegformerFeatureExtractor.from_pretrained(\"nvidia/segformer-b0-finetuned-cityscapes-512-1024\", size = {\"height\": 512,\"width\": 1024})\n",
    "# feature_extractor = SegformerFeatureExtractor.from_pretrained(\"nvidia/segformer-b0-finetuned-cityscapes-512-1024\")\n",
    "# model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b0-finetuned-cityscapes-512-1024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BaseModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(BaseModel, self).__init__()\n",
    "#         self.backbone = model\n",
    "#         self.conv1 = nn.Conv2d(in_channels=19, out_channels=13, kernel_size=1, padding=0)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.backbone(**x).logits\n",
    "#         x = self.conv1(torch.tensor(x))\n",
    "        \n",
    "#         return x\n",
    "    \n",
    "# model = BaseModel()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerConfig\n",
    "\n",
    "# Segformer 모델을 불러오고 구성을 수정합니다.\n",
    "config = SegformerConfig.from_pretrained(\"nvidia/segformer-b0-finetuned-cityscapes-512-1024\")\n",
    "# config.num_labels = 13  # 분할 클래스 수에 맞게 수정\n",
    "config.num_labels = 19  # 이거 테스트 해보자. 일단 그냥 19로 하고 아래쪽에 conv 레이어 한번더 \n",
    "\n",
    "\n",
    "segformer_model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b0-finetuned-cityscapes-512-1024\", config=config, ignore_mismatched_sizes=True)\n",
    "\n",
    "# 분할 모델을 만듭니다.\n",
    "class SegmentationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SegmentationModel, self).__init__()\n",
    "        self.backbone = segformer_model\n",
    "        self.conv1 = nn.Conv2d(in_channels=config.num_labels, out_channels=13, kernel_size=1, padding=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(**x).logits\n",
    "        x = self.conv1(x) # 위랑 연결 \n",
    "        \n",
    "        return x\n",
    "\n",
    "# 모델을 초기화합니다.\n",
    "model = SegmentationModel() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define mIoU for Score >> 가져온 함수여서... batch 사이즈에 대한 고려가 안되어 있을 수 있음\n",
    "def mIoU(pred_mask, mask, smooth=1e-10, n_classes=13):\n",
    "    with torch.no_grad():\n",
    "        pred_mask = F.softmax(pred_mask, dim=1)\n",
    "        pred_mask = torch.argmax(pred_mask, dim=1)\n",
    "        pred_mask = pred_mask.contiguous().view(-1)\n",
    "        mask = mask.contiguous().view(-1)\n",
    "\n",
    "        iou_per_class = []\n",
    "        for clas in range(0, n_classes): #loop per pixel class\n",
    "            true_class = pred_mask == clas\n",
    "            true_label = mask == clas\n",
    "\n",
    "            if true_label.long().sum().item() == 0: #no exist label in this loop\n",
    "                iou_per_class.append(np.nan)\n",
    "            else:\n",
    "                intersect = torch.logical_and(true_class, true_label).sum().float().item()\n",
    "                union = torch.logical_or(true_class, true_label).sum().float().item()\n",
    "\n",
    "                iou = (intersect + smooth) / (union +smooth)\n",
    "                iou_per_class.append(iou)\n",
    "                \n",
    "    return np.nanmean(iou_per_class) , iou_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_score = 0\n",
    "    Check_list = []\n",
    "    with torch.no_grad():\n",
    "        for source , gt in tqdm(iter(val_loader)):\n",
    "            inputs = feature_extractor(images=source, return_tensors=\"pt\")\n",
    "            inputs = inputs.to(device)\n",
    "            gt = gt.long().to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, gt.squeeze(1))\n",
    "            val_loss += loss.item()\n",
    "            a, b = mIoU(outputs, gt)\n",
    "            val_score += a\n",
    "            Check_list.append(b)\n",
    "    Check_list = np.array(Check_list)\n",
    "    print(np.nanmean(Check_list, axis=0))\n",
    "    return val_loss/len(val_loader) , val_score/len(val_loader)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, val_loader, scheduler, device):\n",
    "    # Model load \n",
    "    model = model.to(device) # 그냥 model.to(device)만 하면 저장 안됨\n",
    "\n",
    "    # iou_values = [0.93176631,0.63241147,0.88556955,0.74984703,0.45483095,0.57318848, 0.66932274,0.87954137,0.96864079,0.3430822,0.36345519,0.910648,0.96767449 ]\n",
    "    # iou_array = np.array(iou_values, dtype=np.float32)\n",
    "    # class_weights = 1.0 / iou_array\n",
    "    # class_weights /= class_weights.sum()\n",
    "    # class_weights_tensor = torch.tensor([class_weights], dtype=torch.float32, device=device)\n",
    "    # criterion = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "    # iou_values = [0.93176631, 0.63241147, 0.88556955, 0.74984703, 0.45483095, 0.57318848, 0.66932274, 0.87954137, 0.96864079, 0.3430822, 0.36345519, 0.910648, 0.96767449]\n",
    "    # iou_array = np.array(iou_values, dtype=np.float32)\n",
    "    # class_weights = 1.0 / iou_array\n",
    "    # class_weights /= class_weights.sum()\n",
    "    # class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32, device=device)\n",
    "    \n",
    "    # Define your model, optimizer, and data loaders here\n",
    "    \n",
    "    # Create the CrossEntropyLoss criterion with class weights\n",
    "    # criterion = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "    \n",
    "    # loss function과 optimizer 정의\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    best_score = 0\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(0, CFG['EPOCHS']):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for source , gt in tqdm(train_loader):\n",
    "            \n",
    "            inputs = feature_extractor(images=source, return_tensors=\"pt\")\n",
    "            inputs = inputs.to(device)\n",
    "            gt = gt.long().to(device)\n",
    "            \n",
    "            \n",
    "            optimizer.zero_grad() #! 이건 뭐해주는거지?? 추후에 확인 필\n",
    "            # outputs = model(**inputs)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # logits = outputs.logits\n",
    "            # outputs = outputs.logits.to(\"cpu\").detach().numpy()\n",
    "            # outputs = torch.from_numpy(outputs).float()\n",
    "            # outputs = outputs.to(device)\n",
    "            # outputs.requires_grad_(True)\n",
    "            loss = criterion(outputs, gt.squeeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        _train_loss = train_loss/len(train_loader)\n",
    "    \n",
    "        _val_loss, _val_score = validation(model, criterion, val_loader, device)\n",
    "        print(f'Epoch [{epoch}], Train Loss : [{_train_loss:.5f}] Val Loss : [{_val_loss:.5f}] Val accuracy score : [{_val_score:.5f}]')\n",
    "         \n",
    "        if scheduler is not None:\n",
    "            scheduler.step(_val_score)\n",
    "        \n",
    "        if best_score < _val_score:\n",
    "            best_score = _val_score\n",
    "            best_model = model\n",
    "            torch.save(best_model.state_dict(), \"./models/NM.pt\")\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\EHmin\\Camera-Invariant-Domain-Adaptation\\Test_Another\\Fintune_segformer_model_change.ipynb Cell 24\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/EHmin/Camera-Invariant-Domain-Adaptation/Test_Another/Fintune_segformer_model_change.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(params \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mparameters(), lr \u001b[39m=\u001b[39m CFG[\u001b[39m\"\u001b[39m\u001b[39mLEARNING_RATE\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/EHmin/Camera-Invariant-Domain-Adaptation/Test_Another/Fintune_segformer_model_change.ipynb#X33sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m scheduler \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mReduceLROnPlateau(optimizer, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m'\u001b[39m, factor\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, threshold_mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mabs\u001b[39m\u001b[39m'\u001b[39m, min_lr\u001b[39m=\u001b[39m\u001b[39m1e-8\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/EHmin/Camera-Invariant-Domain-Adaptation/Test_Another/Fintune_segformer_model_change.ipynb#X33sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m infer_model \u001b[39m=\u001b[39m train(model, optimizer, train_loader, val_loader, scheduler, device)\n",
      "\u001b[1;32mc:\\Users\\EHmin\\Camera-Invariant-Domain-Adaptation\\Test_Another\\Fintune_segformer_model_change.ipynb Cell 24\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/EHmin/Camera-Invariant-Domain-Adaptation/Test_Another/Fintune_segformer_model_change.ipynb#X33sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/EHmin/Camera-Invariant-Domain-Adaptation/Test_Another/Fintune_segformer_model_change.ipynb#X33sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m train_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/EHmin/Camera-Invariant-Domain-Adaptation/Test_Another/Fintune_segformer_model_change.ipynb#X33sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mfor\u001b[39;00m source , gt \u001b[39min\u001b[39;00m tqdm(train_loader):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/EHmin/Camera-Invariant-Domain-Adaptation/Test_Another/Fintune_segformer_model_change.ipynb#X33sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     inputs \u001b[39m=\u001b[39m feature_extractor(images\u001b[39m=\u001b[39msource, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/EHmin/Camera-Invariant-Domain-Adaptation/Test_Another/Fintune_segformer_model_change.ipynb#X33sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     inputs \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\EHmin\\anaconda3\\envs\\EHmin\\lib\\site-packages\\tqdm\\notebook.py:254\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    253\u001b[0m     it \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m(tqdm_notebook, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__iter__\u001b[39m()\n\u001b[1;32m--> 254\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m it:\n\u001b[0;32m    255\u001b[0m         \u001b[39m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[0;32m    256\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m    257\u001b[0m \u001b[39m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\EHmin\\anaconda3\\envs\\EHmin\\lib\\site-packages\\tqdm\\std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\EHmin\\anaconda3\\envs\\EHmin\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\EHmin\\anaconda3\\envs\\EHmin\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\EHmin\\anaconda3\\envs\\EHmin\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\EHmin\\anaconda3\\envs\\EHmin\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32mc:\\Users\\EHmin\\Camera-Invariant-Domain-Adaptation\\Test_Another\\Fintune_segformer_model_change.ipynb Cell 24\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/EHmin/Camera-Invariant-Domain-Adaptation/Test_Another/Fintune_segformer_model_change.ipynb#X33sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/EHmin/Camera-Invariant-Domain-Adaptation/Test_Another/Fintune_segformer_model_change.ipynb#X33sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     img_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msource[idx]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/EHmin/Camera-Invariant-Domain-Adaptation/Test_Another/Fintune_segformer_model_change.ipynb#X33sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     image \u001b[39m=\u001b[39m cv\u001b[39m.\u001b[39;49mimread(img_path)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/EHmin/Camera-Invariant-Domain-Adaptation/Test_Another/Fintune_segformer_model_change.ipynb#X33sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfer:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/EHmin/Camera-Invariant-Domain-Adaptation/Test_Another/Fintune_segformer_model_change.ipynb#X33sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m         label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgt[idx]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, threshold_mode='abs', min_lr=1e-8, verbose=True)\n",
    "\n",
    "infer_model = train(model, optimizer, train_loader, val_loader, scheduler, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('../DataPreprocessing/front_OR_back.csv')\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(source = df_test['source'].values ,gt = df_test['label'].values , transform=None,t2=None, infer=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_0 = np.load(\"../DataPreprocessing/mask0.npy\")\n",
    "mask_1 = np.load(\"../DataPreprocessing/mask1.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # infer_model.eval()\n",
    "    model = model.to(device)\n",
    "    result = []\n",
    "    for images, label in tqdm(test_loader):\n",
    "        # images = images.float().to(device)\n",
    "        # images = images.to(device)\n",
    "        # print(images)\n",
    "        # images = cv.imread(images)\n",
    "        \n",
    "        inputs = feature_extractor(images=images, return_tensors=\"pt\")\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        outputs = nn.functional.interpolate(outputs,size=(540,960), mode='bilinear',align_corners=False)\n",
    "        outputs = outputs.argmax( dim=1).to(\"cpu\").numpy()\n",
    "        \n",
    "        flag = True\n",
    "        for pred , l in zip(outputs,label):\n",
    "            new_pred = np.array(pred)\n",
    "            new_pred = new_pred.astype(np.uint8)\n",
    "            # print(new_pred.shape)\n",
    "            \n",
    "            if l == 0:\n",
    "                new_pred[~mask_0] = 12\n",
    "            else:\n",
    "                new_pred[~mask_1] = 12\n",
    "             \n",
    "            if flag:   \n",
    "                np.save('./test_img.npy', new_pred)\n",
    "                flag = False\n",
    "                \n",
    "            for class_id in range(12):\n",
    "                class_mask = (new_pred == class_id).astype(np.uint8)\n",
    "                if np.sum(class_mask) > 0: # 마스크가 존재하는 경우 encode\n",
    "                    mask_rle = rle_encode(class_mask)\n",
    "                    result.append(mask_rle)\n",
    "                else: # 마스크가 존재하지 않는 경우 -1\n",
    "                    result.append(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submisssion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('../Data/sample_submission.csv')\n",
    "submit['mask_rle'] = result\n",
    "submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('./NM.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EHmin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
