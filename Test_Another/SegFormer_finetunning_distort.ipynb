{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import glob\n",
    "import cv2 as cv\n",
    "import random\n",
    "import os\n",
    "from fisheye import ApplyFishEye\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "import PIL.ImageOps    \n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.utils\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import timm\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RLE 인코딩 함수\n",
    "def rle_encode(mask):\n",
    "    pixels = mask.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = 'a'\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "CFG = {\n",
    "    'IMG_SIZE':224,\n",
    "    'EPOCHS':10,\n",
    "    'LEARNING_RATE':3e-5,\n",
    "    # 'LEARNING_RATE':10,\n",
    "    'BATCH_SIZE':8,\n",
    "    'SEED':41\n",
    "}\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#폴더 이동시 경로 수정이 필요할 수 있음 \n",
    "train_source = glob.glob(\"../Data/train_source_image/*\")\n",
    "val_source = glob.glob(\"../Data/val_source_image/*\")\n",
    "train_gt = glob.glob(\"../Data/train_source_gt/*\")\n",
    "val_gt = glob.glob(\"../Data/val_source_gt/*\")\n",
    "\n",
    "train_source += val_source\n",
    "train_gt += val_gt\n",
    "\n",
    "# glob 이후에 정렬이 안되어 있기 때문에, source - gt matching을 위해 정렬\n",
    "train_source.sort()\n",
    "train_gt.sort()\n",
    "\n",
    "print(train_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF 생성 \n",
    "df = pd.DataFrame(columns=['source','gt'])\n",
    "df['source'] = train_source\n",
    "df['gt'] = train_gt\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_back = np.load(\"../DataPreprocessing/mask0_1024_512.npy\")\n",
    "img_back = cv.imread('../DataPreprocessing/back.png')\n",
    "\n",
    "mask_front = np.load(\"../DataPreprocessing/mask1_1024_512.npy\")\n",
    "img_front = cv.imread('../DataPreprocessing/front.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, source, gt, transform=None,t2= None, infer=False):\n",
    "        self.source = source\n",
    "        self.gt = gt\n",
    "        self.transform = transform\n",
    "        self.t2 = t2\n",
    "        self.infer = infer\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.source[idx]\n",
    "        image = cv.imread(img_path)\n",
    "        if idx % 2 == 0:\n",
    "            image = ApplyFishEye(image, mask_np = mask_back, mask_image=img_back, is_target= False)\n",
    "        else :\n",
    "            image = ApplyFishEye(image, mask_np = mask_front, mask_image=img_front, is_target= False)\n",
    "            \n",
    "        \n",
    "        #추론의 경우 이때 self.gt[idx] 은 0,1 의 index\n",
    "        if self.infer:\n",
    "            image = cv.imread(img_path)\n",
    "            label = self.gt[idx]\n",
    "            if self.transform:\n",
    "                image = self.transform(image=image)['image']\n",
    "            return image, label\n",
    "        \n",
    "        \n",
    "        mask_path = self.gt[idx]\n",
    "        mask = cv.imread(mask_path)\n",
    "        mask[mask == 255] = 12 #/ 배경을 픽셀값 12로 간주 이거 원래 없던 값!\n",
    "        mask[mask == 0] = 13 # 잠시 0값을 다른 값으로 변경 \n",
    "        if idx % 2 == 0:\n",
    "            mask = ApplyFishEye(mask, mask_np = mask_back, mask_image=img_back, is_target= True)\n",
    "        else :\n",
    "            mask = ApplyFishEye(mask, mask_np = mask_front, mask_image=img_front, is_target= True)\n",
    "        mask[mask == 13] = 0\n",
    "        \n",
    "\n",
    "        if self.transform: # 알부네이션 먹이이는 형식으로 진행 \n",
    "            augmented = self.transform(image=image, mask = mask) \n",
    "            image = augmented['image']\n",
    "            \n",
    "        if self.t2: #현재 들어가는 구조가 다름 totensor 진행해줌\n",
    "            augmented = self.t2(image=image, mask = mask) \n",
    "            mask = augmented['mask']\n",
    "            \n",
    "            \n",
    "        return image, mask\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.source)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfrom - Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose(\n",
    "    [   \n",
    "        A.Resize(512, 1024),\n",
    "        # A.Normalize(),\n",
    "        # ToTensorV2()\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_si = A.Compose(\n",
    "    [   \n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.Rotate(limit=30, p=0.5),\n",
    "        A.OneOf([\n",
    "            A.Defocus(always_apply=False, p=0.5, radius=(3, 10), alias_blur=(0.1, 0.5)),\n",
    "            A.RandomBrightnessContrast(always_apply=False, p=0.5)\n",
    "        ]),\n",
    "        A.Resize(512, 1024),\n",
    "        # A.Normalize(),\n",
    "        # ToTensorV2()\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "transform_gt = A.Compose(\n",
    "    [   \n",
    "        A.Resize(128, 256), # 반대일 수도 있음 \n",
    "        A.Normalize(),\n",
    "        ToTensorV2()\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, _, _ = train_test_split(df, _, test_size=0.3, random_state=CFG['SEED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset1= CustomDataset(source = train['source'].values, gt = train['gt'].values, transform=transform,t2 = transform_gt, infer=False)\n",
    "# train_dataset2 = CustomDataset(source = train['source'].values, gt = train['gt'].values, transform=transform_si,t2 = transform_gt, infer=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset1, batch_size=4, shuffle=True, num_workers=0)\n",
    "\n",
    "val_dataset = CustomDataset(source = val['source'].values, gt = val['gt'].values, transform=None,t2 = transform_gt, infer=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "### SMP API\n",
    "\n",
    "- model.encoder - pretrained backbone to extract features of different spatial resolution\n",
    "- model.decoder - depends on models architecture (Unet/Linknet/PSPNet/FPN)\n",
    "- model.segmentation_head - last block to produce required number of mask channels (include also optional upsampling and activation)\n",
    "- model.classification_head - optional block which create classification head on top of encoder\n",
    "- model.forward(x) - sequentially pass x through model`s encoder, decoder and segmentation head (and classification head if specified)\n",
    "\n",
    "### Model Param\n",
    " - Docs - https://www.kaggle.com/code/ligtfeather/semantic-segmentation-is-easy-with-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\n",
    "from PIL import Image\n",
    "import requests\n",
    "import cv2 as cv\n",
    "\n",
    "feature_extractor = SegformerFeatureExtractor.from_pretrained(\"nvidia/segformer-b0-finetuned-cityscapes-512-1024\", size = {\"height\": 512,\"width\": 1024})\n",
    "# feature_extractor = SegformerFeatureExtractor.from_pretrained(\"nvidia/segformer-b0-finetuned-cityscapes-512-1024\")\n",
    "# model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b0-finetuned-cityscapes-512-1024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BaseModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(BaseModel, self).__init__()\n",
    "#         self.backbone = model\n",
    "#         self.conv1 = nn.Conv2d(in_channels=19, out_channels=13, kernel_size=1, padding=0)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.backbone(**x).logits\n",
    "#         x = self.conv1(torch.tensor(x))\n",
    "        \n",
    "#         return x\n",
    "    \n",
    "# model = BaseModel()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerConfig\n",
    "\n",
    "# Segformer 모델을 불러오고 구성을 수정합니다.\n",
    "config = SegformerConfig.from_pretrained(\"nvidia/segformer-b0-finetuned-cityscapes-512-1024\")\n",
    "# config.num_labels = 13  # 분할 클래스 수에 맞게 수정\n",
    "config.num_labels = 19  # 이거 테스트 해보자. 일단 그냥 19로 하고 아래쪽에 conv 레이어 한번더 \n",
    "\n",
    "\n",
    "segformer_model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b0-finetuned-cityscapes-512-1024\", config=config, ignore_mismatched_sizes=True)\n",
    "# segformer_model = torch.load('./models/pretrained_undistort.pt')\n",
    "\n",
    "# 분할 모델을 만듭니다.\n",
    "class SegmentationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SegmentationModel, self).__init__()\n",
    "        self.backbone = segformer_model\n",
    "        self.conv1 = nn.Conv2d(in_channels=config.num_labels, out_channels=13, kernel_size=1, padding=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(**x).logits\n",
    "        x = self.conv1(x) # 위랑 연결 \n",
    "        \n",
    "        return x\n",
    "\n",
    "# 모델을 초기화합니다.\n",
    "model = SegmentationModel() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 추가적으로 distort된 것에 대한 증강을 하기 위해 사용한거임! 돌릴때 주의 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('./models/pretrained_undistort.pt'))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define mIoU for Score >> 가져온 함수여서... batch 사이즈에 대한 고려가 안되어 있을 수 있음\n",
    "def mIoU(pred_mask, mask, smooth=1e-10, n_classes=13):\n",
    "    with torch.no_grad():\n",
    "        pred_mask = F.softmax(pred_mask, dim=1)\n",
    "        pred_mask = torch.argmax(pred_mask, dim=1)\n",
    "        pred_mask = pred_mask.contiguous().view(-1)\n",
    "        mask = mask.contiguous().view(-1)\n",
    "\n",
    "        iou_per_class = []\n",
    "        for clas in range(0, n_classes): #loop per pixel class\n",
    "            true_class = pred_mask == clas\n",
    "            true_label = mask == clas\n",
    "\n",
    "            if true_label.long().sum().item() == 0: #no exist label in this loop\n",
    "                iou_per_class.append(np.nan)\n",
    "            else:\n",
    "                intersect = torch.logical_and(true_class, true_label).sum().float().item()\n",
    "                union = torch.logical_or(true_class, true_label).sum().float().item()\n",
    "\n",
    "                iou = (intersect + smooth) / (union +smooth)\n",
    "                iou_per_class.append(iou)\n",
    "                \n",
    "    return np.nanmean(iou_per_class) , iou_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_score = 0\n",
    "    Check_list = []\n",
    "    with torch.no_grad():\n",
    "        for source , gt in tqdm(iter(val_loader)):\n",
    "            inputs = feature_extractor(images=source, return_tensors=\"pt\")\n",
    "            inputs = inputs.to(device)\n",
    "            gt = gt.long().to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, gt.squeeze(1))\n",
    "            val_loss += loss.item()\n",
    "            a, b = mIoU(outputs, gt)\n",
    "            val_score += a\n",
    "            Check_list.append(b)\n",
    "    Check_list = np.array(Check_list)\n",
    "    print(np.nanmean(Check_list, axis=0))\n",
    "    return val_loss/len(val_loader) , val_score/len(val_loader)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, val_loader, scheduler, device):\n",
    "    # Model load \n",
    "    model = model.to(device) # 그냥 model.to(device)만 하면 저장 안됨\n",
    "\n",
    "    # iou_values = [0.93176631,0.63241147,0.88556955,0.74984703,0.45483095,0.57318848, 0.66932274,0.87954137,0.96864079,0.3430822,0.36345519,0.910648,0.96767449 ]\n",
    "    # iou_array = np.array(iou_values, dtype=np.float32)\n",
    "    # class_weights = 1.0 / iou_array\n",
    "    # class_weights /= class_weights.sum()\n",
    "    # class_weights_tensor = torch.tensor([class_weights], dtype=torch.float32, device=device)\n",
    "    # criterion = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "    # iou_values = [0.93176631, 0.63241147, 0.88556955, 0.74984703, 0.45483095, 0.57318848, 0.66932274, 0.87954137, 0.96864079, 0.3430822, 0.36345519, 0.910648, 0.96767449]\n",
    "    # iou_array = np.array(iou_values, dtype=np.float32)\n",
    "    # class_weights = 1.0 / iou_array\n",
    "    # class_weights /= class_weights.sum()\n",
    "    # class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32, device=device)\n",
    "    \n",
    "    # Define your model, optimizer, and data loaders here\n",
    "    \n",
    "    # Create the CrossEntropyLoss criterion with class weights\n",
    "    # criterion = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "    \n",
    "    # loss function과 optimizer 정의\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    best_score = 0\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(0, CFG['EPOCHS']):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for source , gt in tqdm(train_loader):\n",
    "            # source = cv.cvtColor(source, cv.COLOR_RGB2BGR)\n",
    "            inputs = feature_extractor(images=source, return_tensors=\"pt\")\n",
    "            inputs = inputs.to(device)\n",
    "            gt = gt.long().to(device)\n",
    "            \n",
    "            \n",
    "            optimizer.zero_grad() #! 이건 뭐해주는거지?? 추후에 확인 필\n",
    "            # outputs = model(**inputs)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # logits = outputs.logits\n",
    "            # outputs = outputs.logits.to(\"cpu\").detach().numpy()\n",
    "            # outputs = torch.from_numpy(outputs).float()\n",
    "            # outputs = outputs.to(device)\n",
    "            # outputs.requires_grad_(True)\n",
    "            loss = criterion(outputs, gt.squeeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        _train_loss = train_loss/len(train_loader)\n",
    "    \n",
    "        _val_loss, _val_score = validation(model, criterion, val_loader, device)\n",
    "        print(f'Epoch [{epoch}], Train Loss : [{_train_loss:.5f}] Val Loss : [{_val_loss:.5f}] Val accuracy score : [{_val_score:.5f}]')\n",
    "         \n",
    "        if scheduler is not None:\n",
    "            scheduler.step(_val_score)\n",
    "        \n",
    "        if best_score < _val_score:\n",
    "            best_score = _val_score\n",
    "            best_model = model\n",
    "            torch.save(best_model.state_dict(), \"./models/Segformer_fintunning_distort_10Epoch.pt\")\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, threshold_mode='abs', min_lr=1e-8, verbose=True)\n",
    "\n",
    "infer_model = train(model, optimizer, train_loader, val_loader, scheduler, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('../DataPreprocessing/front_OR_back.csv')\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(source = df_test['source'].values ,gt = df_test['label'].values , transform=None,t2=None, infer=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_0 = np.load(\"../DataPreprocessing/mask0.npy\")\n",
    "mask_1 = np.load(\"../DataPreprocessing/mask1.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # infer_model.eval()\n",
    "    model = model.to(device)\n",
    "    result = []\n",
    "    for images, label in tqdm(test_loader):\n",
    "        # images = images.float().to(device)\n",
    "        # images = images.to(device)\n",
    "        # print(images)\n",
    "        # images = cv.imread(images)\n",
    "        \n",
    "        inputs = feature_extractor(images=images, return_tensors=\"pt\")\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        outputs = nn.functional.interpolate(outputs,size=(540,960), mode='bilinear',align_corners=False)\n",
    "        outputs = outputs.argmax( dim=1).to(\"cpu\").numpy()\n",
    "        \n",
    "        flag = True\n",
    "        for pred , l in zip(outputs,label):\n",
    "            new_pred = np.array(pred)\n",
    "            new_pred = new_pred.astype(np.uint8)\n",
    "            # print(new_pred.shape)\n",
    "            \n",
    "            if l == 0:\n",
    "                new_pred[~mask_0] = 12\n",
    "            else:\n",
    "                new_pred[~mask_1] = 12\n",
    "             \n",
    "            if flag:   \n",
    "                np.save('./test_img.npy', new_pred)\n",
    "                flag = False\n",
    "                \n",
    "            for class_id in range(12):\n",
    "                class_mask = (new_pred == class_id).astype(np.uint8)\n",
    "                if np.sum(class_mask) > 0: # 마스크가 존재하는 경우 encode\n",
    "                    mask_rle = rle_encode(class_mask)\n",
    "                    result.append(mask_rle)\n",
    "                else: # 마스크가 존재하지 않는 경우 -1\n",
    "                    result.append(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submisssion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('../Data/sample_submission.csv')\n",
    "submit['mask_rle'] = result\n",
    "submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('./Segformer_fintunning_distort_10Epoch.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EHmin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
