{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import glob\n",
    "import cv2 as cv\n",
    "import random\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "import PIL.ImageOps    \n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.utils\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import timm\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RLE 인코딩 함수\n",
    "def rle_encode(mask):\n",
    "    pixels = mask.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = 'a'\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "CFG = {\n",
    "    'IMG_SIZE':224,\n",
    "    'EPOCHS':30,\n",
    "    'LEARNING_RATE':3e-4,\n",
    "    # 'LEARNING_RATE':10,\n",
    "    'BATCH_SIZE':8,\n",
    "    'SEED':41\n",
    "}\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#폴더 이동시 경로 수정이 필요할 수 있음 \n",
    "train_source = glob.glob(\"../Data/train_source_image/*\")\n",
    "val_source = glob.glob(\"../Data/val_source_image/*\")\n",
    "train_gt = glob.glob(\"../Data/train_source_gt/*\")\n",
    "val_gt = glob.glob(\"../Data/val_source_gt/*\")\n",
    "\n",
    "train_source += val_source\n",
    "train_gt += val_gt\n",
    "\n",
    "# glob 이후에 정렬이 안되어 있기 때문에, source - gt matching을 위해 정렬\n",
    "train_source.sort()\n",
    "train_gt.sort()\n",
    "\n",
    "print(train_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF 생성 \n",
    "df_seg = pd.DataFrame(columns=['source','gt'])\n",
    "df_seg['source'] = train_source\n",
    "df_seg['gt'] = train_gt\n",
    "df_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#폴더 이동시 경로 수정이 필요할 수 있음 \n",
    "train_source_l = glob.glob(\"../Data/train_source_image/*\")\n",
    "val_source_l = glob.glob(\"../Data/val_source_image/*\")\n",
    "train_target_l = glob.glob(\"../Data/train_target_image/*\")\n",
    "\n",
    "length_s = len(train_source_l) + len(val_source_l)\n",
    "length_t = len(train_target_l)\n",
    "label = [0 for _ in range(length_s)] + [1 for _ in range(length_t)]\n",
    "\n",
    "train_source_l = train_source_l + val_source_l + train_target_l\n",
    "\n",
    "# glob 이후에 정렬이 안되어 있기 때문에, source - gt matching을 위해 정렬\n",
    "train_source_l.sort()\n",
    "\n",
    "print(train_source_l)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF 생성 \n",
    "df_domain_adpt = pd.DataFrame(columns=['source','label'])\n",
    "df_domain_adpt['source'] = train_source_l\n",
    "df_domain_adpt['label'] = label\n",
    "df_domain_adpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_seg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset_seg(Dataset):\n",
    "    def __init__(self, source, gt, transform=None, infer=False):\n",
    "        self.source = source\n",
    "        self.gt = gt\n",
    "        self.transform = transform\n",
    "        self.infer = infer\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.source[idx]\n",
    "        image = cv.imread(img_path)\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.infer:\n",
    "            if self.transform:\n",
    "                image = self.transform(image=image)['image']\n",
    "            return image\n",
    "        \n",
    "        mask_path = self.gt[idx]\n",
    "        mask = cv.imread(mask_path, cv.IMREAD_GRAYSCALE)\n",
    "        mask[mask == 255] = 12 #/ 배경을 픽셀값 12로 간주 이거 원래 없던 값!\n",
    "\n",
    "        if self.transform: # 알부네이션 먹이이는 형식으로 진행 \n",
    "            augmented = self.transform(image=image, mask=mask) \n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "            \n",
    "        return image, mask\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset_domain_adpt(Dataset): # 도메인 adaptive 한 형식으로 진행하기 위해서!\n",
    "    def __init__(self, source, label, transform=None, infer=False):\n",
    "        self.source = source\n",
    "        self.label = label\n",
    "        self.transform = transform\n",
    "        self.infer = infer\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.source[idx]\n",
    "        image = cv.imread(img_path)\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "        \n",
    "        # if self.infer: #/해당 Data Loader는 사실상 학습에만 쓰여서 애당 코드 필요 없음!\n",
    "        #     if self.transform:\n",
    "        #         image = self.transform(image=image)['image']\n",
    "        #     return image\n",
    "        \n",
    "        label = self.label[idx]\n",
    "        \n",
    "\n",
    "        if self.transform: # 알부네이션 먹이이는 형식으로 진행 \n",
    "            augmented = self.transform(image=image) \n",
    "            image = augmented['image']\n",
    "            \n",
    "        return image, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfrom - Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose(\n",
    "    [   \n",
    "        A.Resize(224, 224),\n",
    "        A.Normalize(),\n",
    "        ToTensorV2()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seg, val_seg, _, _ = train_test_split(df_seg, _, test_size=0.2, random_state=CFG['SEED'])\n",
    "train_domain_adpt, val_domain_adpt, _, _ = train_test_split(df_domain_adpt, df_domain_adpt['label'], test_size=0.2, random_state=CFG['SEED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_seg = CustomDataset_seg(source = train_seg['source'].values, gt = train_seg['gt'].values, transform=transform, infer=False)\n",
    "train_loader_seg = DataLoader(train_dataset_seg, batch_size=16, shuffle=True, num_workers=0)\n",
    "\n",
    "val_dataset_seg = CustomDataset_seg(source = val_seg['source'].values, gt = val_seg['gt'].values, transform=transform, infer=False)\n",
    "val_loader_seg = DataLoader(val_dataset_seg, batch_size=16, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_domain_adpt = CustomDataset_domain_adpt(source = train_domain_adpt['source'].values, label = train_domain_adpt['label'].values, transform=transform, infer=False)\n",
    "train_loader_domain_adpt = DataLoader(train_dataset_domain_adpt, batch_size=16, shuffle=True, num_workers=0)\n",
    "\n",
    "val_dataset_domain_adpt = CustomDataset_domain_adpt(source = val_domain_adpt['source'].values, label = val_domain_adpt['label'].values, transform=transform, infer=False)\n",
    "val_loader_domain_adpt = DataLoader(val_dataset_domain_adpt, batch_size=16, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "### SMP API\n",
    "\n",
    "- model.encoder - pretrained backbone to extract features of different spatial resolution\n",
    "- model.decoder - depends on models architecture (Unet/Linknet/PSPNet/FPN)\n",
    "- model.segmentation_head - last block to produce required number of mask channels (include also optional upsampling and activation)\n",
    "- model.classification_head - optional block which create classification head on top of encoder\n",
    "- model.forward(x) - sequentially pass x through model`s encoder, decoder and segmentation head (and classification head if specified)\n",
    "\n",
    "### Model Param\n",
    " - Docs - https://www.kaggle.com/code/ligtfeather/semantic-segmentation-is-easy-with-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_params=dict(\n",
    "    pooling='avg',             # one of 'avg', 'max'\n",
    "    dropout=0.5,               # dropout ratio, default is None\n",
    "    activation='sigmoid',      # activation function, default is None\n",
    "    classes=13,                 # define number of output labels\n",
    ")\n",
    "\n",
    "# model = smp.Unet('mobilenet_v2', encoder_weights='imagenet', classes=13, activation=None, encoder_depth=5, decoder_channels=[256, 128, 64, 32, 16] , aux_params=aux_params)\n",
    "# model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_params=dict(\n",
    "    pooling='max',             # one of 'avg', 'max'\n",
    "    dropout=0.5,               # dropout ratio, default is None\n",
    "    activation='sigmoid',      # activation function, default is None\n",
    "    classes=13,                 # define number of output labels\n",
    ")\n",
    "\n",
    "# from torch.autograd import Function\n",
    "\n",
    "# class ReverseLayerF(Function):\n",
    "#     @staticmethod\n",
    "#     def forward(ctx, x, alpha):\n",
    "#         ctx.alpha = alpha\n",
    "\n",
    "#         return x.view_as(x)\n",
    "\n",
    "#     @staticmethod\n",
    "#     def backward(ctx, grad_output):\n",
    "#         output = grad_output.neg() * ctx.alpha\n",
    "\n",
    "#         return output, None\n",
    "from torch.autograd import Function\n",
    "\n",
    "class ReverseLayerF(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return [-x for x in grad_output]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "class DANN(pl.LightningModule):\n",
    "    \n",
    "    def __init__( self, arch, encoder_name, in_channels, out_classes, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.model = smp.create_model(\n",
    "            arch, encoder_name=encoder_name, in_channels=in_channels, classes=out_classes, **kwargs\n",
    "        )\n",
    "        \n",
    "        self.AdaptiveAvgPool2d = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.Flatten = nn.Flatten(start_dim=1, end_dim=-1)\n",
    "        self.Dropout = nn.Dropout(p=0.5, inplace=True)\n",
    "        self.Classifier = nn.Linear(in_features=1280, out_features=2, bias=True)\n",
    "        self.Activation = nn.Sigmoid()\n",
    "        \n",
    "    \n",
    "    def forward(self, x, flag):\n",
    "        if flag:\n",
    "            x = self.model.encoder(x)\n",
    "            x = self.model.decoder(*x)\n",
    "            x = self.model.segmentation_head(x)\n",
    "            return x\n",
    "        \n",
    "        else:\n",
    "            x = self.model.encoder(x)\n",
    "            x = ReverseLayerF.apply(x, 1)\n",
    "            x = self.AdaptiveAvgPool2d(x[-1])\n",
    "            x = self.Flatten(x)\n",
    "            x = self.Dropout(x)\n",
    "            x = self.Classifier(x)\n",
    "            x = self.Activation(x)\n",
    "            return x\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DANN(\"Unet\", \"mobilenet_v2\", in_channels=3, out_classes=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define mIoU for Score >> 가져온 함수여서... batch 사이즈에 대한 고려가 안되어 있을 수 있음\n",
    "def mIoU(pred_mask, mask, smooth=1e-10, n_classes=13):\n",
    "    with torch.no_grad():\n",
    "        pred_mask = F.softmax(pred_mask, dim=1)\n",
    "        pred_mask = torch.argmax(pred_mask, dim=1)\n",
    "        pred_mask = pred_mask.contiguous().view(-1)\n",
    "        mask = mask.contiguous().view(-1)\n",
    "\n",
    "        iou_per_class = []\n",
    "        for clas in range(0, n_classes): #loop per pixel class\n",
    "            true_class = pred_mask == clas\n",
    "            true_label = mask == clas\n",
    "\n",
    "            if true_label.long().sum().item() == 0: #no exist label in this loop\n",
    "                iou_per_class.append(np.nan)\n",
    "            else:\n",
    "                intersect = torch.logical_and(true_class, true_label).sum().float().item()\n",
    "                union = torch.logical_or(true_class, true_label).sum().float().item()\n",
    "\n",
    "                iou = (intersect + smooth) / (union +smooth)\n",
    "                iou_per_class.append(iou)\n",
    "                \n",
    "    return np.nanmean(iou_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, loss_segmentation, loss_classification, val_loader_seg,val_loader_domain_adpt, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_score = 0\n",
    "    preds, true_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for  (source , gt), (source_domain, label) in tqdm(zip(val_loader_seg, val_loader_domain_adpt)):\n",
    "            source = source.float().to(device)\n",
    "            gt = gt.long().to(device)\n",
    "            \n",
    "            source_domain = source_domain.float().to(device)\n",
    "            labels = label.type(torch.LongTensor).to(device)\n",
    "            \n",
    "            outputs= model(source, True)\n",
    "            output_c = model(source_domain, False)\n",
    "            \n",
    "            outputs.requires_grad_(True)\n",
    "            output_c.requires_grad_(True)\n",
    "            loss_s = loss_segmentation(outputs, gt.squeeze(1))\n",
    "            loss_d = loss_classification(output_c, labels)\n",
    "            loss = loss_s + loss_d\n",
    "            \n",
    "            preds += output_c.detach().argmax(1).cpu().numpy().tolist()\n",
    "            true_labels += labels.detach().cpu().numpy().tolist()\n",
    "            \n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            val_score += mIoU(outputs, gt)\n",
    "        \n",
    "        val_score_acc = accuracy_score(true_labels, preds)\n",
    "    \n",
    "    return val_loss/len(val_loader_seg) , val_score/len(val_loader_seg), val_score_acc\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader_seg, val_loader_seg, train_loader_domain_adpt, val_loader_domain_adpt, scheduler, device):\n",
    "    # Model load \n",
    "    model = model.to(device) # 그냥 model.to(device)만 하면 저장 안됨\n",
    "\n",
    "    # loss function과 optimizer 정의\n",
    "    loss_segmentation = torch.nn.CrossEntropyLoss()\n",
    "    loss_classification = torch.nn.CrossEntropyLoss()\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=CFG['LEARNING_RATE'])\n",
    "    # 이거 밖에서 선언할거임 \n",
    "    \n",
    "    best_score = 0\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(0, CFG['EPOCHS']):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for (source , gt), (source_domain, label) in tqdm(zip(train_loader_seg, train_loader_domain_adpt)):\n",
    "            source = source.float().to(device)\n",
    "            gt = gt.long().to(device)\n",
    "            \n",
    "            source_domain = source_domain.float().to(device)\n",
    "            labels = label.type(torch.LongTensor).to(device)\n",
    "            \n",
    "            optimizer.zero_grad() #! 이건 뭐해주는거지?? 추후에 확인 필\n",
    "            outputs  = model(source, True)\n",
    "            output_c = model(source_domain, False)\n",
    "            \n",
    "            outputs.requires_grad_(True)\n",
    "            output_c.requires_grad_(True)\n",
    "            \n",
    "            loss_s = loss_segmentation(outputs, gt.squeeze(1))\n",
    "            loss_d = loss_classification(output_c, labels)\n",
    "            ###\n",
    "            loss = (loss_s + loss_d)/2\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss_s.item()\n",
    "            \n",
    "        _train_loss = train_loss/len(train_loader_seg)\n",
    "        _val_loss, _val_score, val_score_acc = validation(model, loss_segmentation, loss_classification, val_loader_seg,val_loader_domain_adpt, device)\n",
    "        print(f'Epoch [{epoch}], Train Loss : [{_train_loss:.5f}] Val Loss : [{_val_loss:.5f}] Val accuracy score : [{_val_score:.5f}] Val accuracy score : [{val_score_acc:.5f}]')\n",
    "         \n",
    "        if scheduler is not None:\n",
    "            scheduler.step(_val_score)\n",
    "        \n",
    "        if best_score < _val_score:\n",
    "            best_score = _val_score\n",
    "            best_model = model\n",
    "            torch.save(best_model.state_dict(), \"./models/DANN_mobile.pt\")\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model()\n",
    "# model.load_state_dict(torch.load('path'))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, threshold_mode='abs', min_lr=1e-8, verbose=True)\n",
    "\n",
    "infer_model = train(model, optimizer, train_loader_seg, val_loader_seg,train_loader_domain_adpt, val_loader_domain_adpt, scheduler, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#폴더 이동시 경로 수정이 필요할 수 있음 \n",
    "test_dataset = glob.glob(\"../Data/test_image/*\")\n",
    "\n",
    "# glob 이후에 정렬이 안되어 있기 때문에, source - gt matching을 위해 정렬\n",
    "test_dataset.sort()\n",
    "\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame(columns=['test'])\n",
    "df_test['test'] = test_dataset\n",
    "\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(source = df_test['test'].values ,gt = _ , transform=transform, infer=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  smp.Unet('mobilenet_v2', encoder_weights='imagenet', classes=13, activation=None, encoder_depth=5, decoder_channels=[256, 128, 64, 32, 16] , aux_params=aux_params)\n",
    "model.load_state_dict(torch.load('./models/smp_base.pt'))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    result = []\n",
    "    for images in tqdm(test_loader):\n",
    "        images = images.float().to(device)\n",
    "        outputs = model(images)\n",
    "        outputs = torch.softmax(outputs[0], dim=1).cpu()\n",
    "        outputs = torch.argmax(outputs, dim=1).numpy()\n",
    "        # batch에 존재하는 각 이미지에 대해서 반복\n",
    "        for pred in outputs:\n",
    "            pred = pred.astype(np.uint8)\n",
    "            pred = Image.fromarray(pred) # 이미지로 변환\n",
    "            pred = pred.resize((960, 540), Image.NEAREST) # 960 x 540 사이즈로 변환\n",
    "            pred = np.array(pred) # 다시 수치로 변환\n",
    "            # class 0 ~ 11에 해당하는 경우에 마스크 형성 / 12(배경)는 제외하고 진행\n",
    "            for class_id in range(12):\n",
    "                class_mask = (pred == class_id).astype(np.uint8)\n",
    "                if np.sum(class_mask) > 0: # 마스크가 존재하는 경우 encode\n",
    "                    mask_rle = rle_encode(class_mask)\n",
    "                    result.append(mask_rle)\n",
    "                else: # 마스크가 존재하지 않는 경우 -1\n",
    "                    result.append(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submisssion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('../Data/sample_submission.csv')\n",
    "submit['mask_rle'] = result\n",
    "submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('./baseline_submit.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EHmin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
